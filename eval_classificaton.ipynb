{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24254d8-fec4-41e8-b8ab-2d0c47569b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb96182-b5bd-4723-9a7b-e9ee65aaa653",
   "metadata": {},
   "source": [
    "Declaration of parameters (you must also add a tag for this cell - parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128a7f5-4294-4578-b301-b947a5a6d832",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# specify substep parameters for interactive run\n",
    "# this cell will be replaced during job run with the parameters from json within params subfolder\n",
    "substep_params={   \n",
    "    \"threshold_accuracy\" : 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195baa6b-5f74-4298-b0a5-95209ab0b3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load pipeline and step parameters - do not edit\n",
    "from sinara.substep import get_pipeline_params, get_step_params\n",
    "pipeline_params = get_pipeline_params(pprint=True)\n",
    "step_params = get_step_params(pprint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b1cd2-1d47-4c7b-89bf-5a235a33f403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define substep interface\n",
    "from sinara.substep import NotebookSubstep, ENV_NAME, PIPELINE_NAME, ZONE_NAME, STEP_NAME, RUN_ID, ENTITY_NAME, ENTITY_PATH, SUBSTEP_NAME\n",
    "\n",
    "substep = NotebookSubstep(pipeline_params, step_params, substep_params)\n",
    "\n",
    "substep.interface(    \n",
    "    tmp_inputs =\n",
    "    [\n",
    "        { ENTITY_NAME: \"inference_result_dataset\" },\n",
    "    ]\n",
    ")\n",
    "\n",
    "substep.print_interface_info()\n",
    "\n",
    "substep.exit_in_visualize_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763846b-0137-4b29-a68c-46d1cb996886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify all notebook wide libraries imports here\n",
    "# Sinara lib imports is left in the place of their usage\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcb829-6442-4169-b73d-1e7a985c5ad0",
   "metadata": {},
   "source": [
    "### Load inference_result_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d09aa-0563-4af0-ab02-12b3a95a707e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_inputs = substep.tmp_inputs()\n",
    "\n",
    "# reading inference detect result dataset \n",
    "predict_test_dataset_file = osp.join(tmp_inputs.inference_result_dataset, 'predict_test_dataset.json')\n",
    "with open(predict_test_dataset_file, 'r') as f:\n",
    "   predict_test_dataset = json.load(f)\n",
    "\n",
    "with open(osp.join(tmp_inputs.inference_result_dataset, 'categories.json'), 'r') as f:\n",
    "   categories = json.load(f)\n",
    "category_names = list(categories.values())\n",
    "n_categories = len(category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d8d2f-b41f-435f-b14f-ca59acec2acd",
   "metadata": {},
   "source": [
    "### Get ground true classification and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba99fc9-2308-4ae5-ad4f-caadb5ff0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels = np.array([sample[\"ground_true_class_index\"] for sample in predict_test_dataset])\n",
    "predict_labels = np.array([sample[\"predict_class_index\"] for sample in predict_test_dataset])\n",
    "predict_scores = np.array([sample[\"predict_class_score\"] for sample in predict_test_dataset])\n",
    "\n",
    "# Convert array of indices to one-hot encoded array\n",
    "ground_truth_labels_one_hot = np.zeros((ground_truth_labels.size, n_categories))\n",
    "ground_truth_labels_one_hot[np.arange(ground_truth_labels.size), ground_truth_labels] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de52261-33b5-4c86-82cf-19690c9b49e7",
   "metadata": {},
   "source": [
    "### Evaluate the test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4475d-81f5-4388-a39d-d257b3831e09",
   "metadata": {},
   "source": [
    "#### Eval Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e803a-ee7e-4b20-bd22-567e5d5b6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import plot_precision_recall\n",
    "\n",
    "plot_precision_recall(ground_truth_labels_one_hot, predict_scores, class_names=category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53763338-e3f5-4a22-9498-d7959f564dc4",
   "metadata": {},
   "source": [
    "#### Eval Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795d87f-fc4d-4e79-9791-fd0c6f9db463",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix = confusion_matrix(ground_truth_labels, predict_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=val_confusion_matrix, display_labels=category_names)\n",
    "disp.plot(include_values=True, cmap=\"viridis\", ax=None, xticks_rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23dee05-298b-44db-b3ef-da866d5efb16",
   "metadata": {},
   "source": [
    "#### Eval Average Precision, Recall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c30fd-a295-44bb-95bf-3aaf1654bded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_report = classification_report(ground_truth_labels, predict_labels, target_names=category_names, output_dict=True)\n",
    "print(classification_report(ground_truth_labels, predict_labels, target_names=category_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045a1fa-6fec-4338-97bd-20880770a271",
   "metadata": {},
   "source": [
    "### Check by metric accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94167121-6b15-4dc8-bd2e-100fb65fb0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold_accuracy = substep_params[\"threshold_accuracy\"]\n",
    "\n",
    "accuracy = val_report[\"accuracy\"]\n",
    "print(f\"accuracy = {accuracy}\")\n",
    "assert accuracy > threshold_accuracy, f\"The calculated Accuracy metric on the test dataset is less than the acceptable value <{threshold_accuracy}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221ecc1-763e-46c7-bb4f-5f2f1dd01a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
